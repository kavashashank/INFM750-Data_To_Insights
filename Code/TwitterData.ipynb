{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, LikesCount, RetweetCount]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "venue_df=pd.DataFrame(columns=['Name','LikesCount','RetweetCount'])\n",
    "print venue_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_api():\n",
    "    consumer_key = 'rqXF1X0aqHngHqi7ohWVBcd4R'\n",
    "    consumer_secret = 'JLy4o5vmgy8ignlFl122Y1qH53t3aCD7Veb5H208rdRxf9c3Gm'\n",
    "    access_token = '3094362888-AuKPdG8xwKyCFKeQlDaryNxywVNcXOJueaijeoU'\n",
    "    access_secret = 'i9lziRbZwoDnPXphtmrob6X9Beb8bAoKitdXEjwNZcQbS'\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    \n",
    "    # load the twitter API via tweepy\n",
    "    return tweepy.API(auth)\n",
    "\n",
    "    \n",
    "def tweet_search(api, query, max_tweets, max_id, since_id, geocode):\n",
    "    ''' Function that takes in a search string 'query', the maximum\n",
    "        number of tweets 'max_tweets', and the minimum (i.e., starting)\n",
    "        tweet id. It returns a list of tweepy.models.Status objects. '''\n",
    "\n",
    "    searched_tweets = []\n",
    "    while len(searched_tweets) < max_tweets:\n",
    "        remaining_tweets = max_tweets - len(searched_tweets)\n",
    "        try:\n",
    "            new_tweets = api.search(q=query, count=remaining_tweets,\n",
    "                                    since_id=str(since_id),\n",
    "                                max_id=str(max_id-1),\n",
    "                                geocode=geocode)\n",
    "            print('found',len(new_tweets),'tweets')\n",
    "            if not new_tweets:\n",
    "                print('no tweets found')\n",
    "                break\n",
    "            searched_tweets.extend(new_tweets)\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError:\n",
    "            print('exception raised, waiting 15 minutes')\n",
    "            print('(until:', dt.datetime.now()+dt.timedelta(minutes=15), ')')\n",
    "            time.sleep(15*60)\n",
    "            break # stop the loop\n",
    "    return searched_tweets, max_id\n",
    "\n",
    "\n",
    "def get_tweet_id(api, date='', days_ago=9, query='a'):\n",
    "    ''' Function that gets the ID of a tweet. This ID can then be\n",
    "        used as a 'starting point' from which to search. The query is\n",
    "        required and has been set to a commonly used word by default.\n",
    "        The variable 'days_ago' has been initialized to the maximum\n",
    "        amount we are able to search back in time (9).'''\n",
    "\n",
    "    if date:\n",
    "        # return an ID from the start of the given day\n",
    "        td = date + dt.timedelta(days=1)\n",
    "        tweet_date = '{0}-{1:0>2}-{2:0>2}'.format(td.year, td.month, td.day)\n",
    "        tweet = api.search(q=query, count=1, until=tweet_date)\n",
    "    else:\n",
    "        # return an ID from __ days ago\n",
    "        td = dt.datetime.now() - dt.timedelta(days=days_ago)\n",
    "        tweet_date = '{0}-{1:0>2}-{2:0>2}'.format(td.year, td.month, td.day)\n",
    "        # get list of up to 10 tweets\n",
    "        tweet = api.search(q=query, count=10, until=tweet_date)\n",
    "        print('search limit (start/stop):',tweet[0].created_at)\n",
    "        # return the id of the first tweet in the list\n",
    "        return tweet[0].id\n",
    "\n",
    "\n",
    "def write_tweets(tweets, filename):\n",
    "    ''' Function that appends tweets to a file. '''\n",
    "\n",
    "    with open(filename, 'a') as f:\n",
    "        for tweet in tweets:\n",
    "            json.dump(tweet._json, f)\n",
    "            f.write('\\n')\n",
    "            \n",
    "    \n",
    "    \n",
    "def populate_tweet_df(tweets):\n",
    "    df = pd.DataFrame(columns=['text','screenname','location','country_code','likes','retweets','retweet_likes','long','latt'])\n",
    "    \n",
    "    retweet_likes_lst=[]\n",
    "    \n",
    "    df['text'] = list(map(lambda tweet: tweet['text'], tweets))\n",
    "    \n",
    "    df['screenname'] = list(map(lambda tweet: tweet['user']['screen_name'], tweets))\n",
    " \n",
    "    for tweet in tweets:\n",
    "        if tweet.get(\"retweeted_status\"):\n",
    "            retweet_likes_lst.append(tweet['retweeted_status']['favorite_count'])\n",
    "        else:\n",
    "            retweet_likes_lst.append(0)\n",
    "    df['retweet_likes']=retweet_likes_lst\n",
    "\n",
    "    df['location'] = list(map(lambda tweet: tweet['user']['location'], tweets))\n",
    " \n",
    "    df['country_code'] = list(map(lambda tweet: tweet['place']['country_code']\n",
    "                                  if tweet['place'] != None else '', tweets))\n",
    "    \n",
    "    df['likes'] = list(map(lambda tweet: tweet['favorite_count'], tweets))\n",
    "     \n",
    "    df['retweets'] = list(map(lambda tweet: tweet['retweet_count'], tweets))    \n",
    " \n",
    "    df['long'] = list(map(lambda tweet: tweet['coordinates']['coordinates'][0]\n",
    "                        if tweet['coordinates'] != None else 'NaN', tweets))\n",
    " \n",
    "    df['latt'] = list(map(lambda tweet: tweet['coordinates']['coordinates'][1]\n",
    "                        if tweet['coordinates'] != None else 'NaN', tweets))\n",
    " \n",
    "    return df\n",
    "\n",
    "def aggregate(data,search_phrase):\n",
    "    print \"Name:\",search_phrase\n",
    "    venue_name_lst.append(search_phrase)\n",
    "    \n",
    "    tweet_likes=data['likes'].sum()\n",
    "    retweet_likes=data['retweet_likes'].sum()\n",
    "    tot_likes=tweet_likes+retweet_likes\n",
    "    likes_lst.append(tot_likes)\n",
    "    \n",
    "    retweets_lst.append(data['retweets'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search phrase = Yellowstone\n",
      "('search limit (start/stop):', datetime.datetime(2017, 10, 1, 23, 59, 59))\n",
      "('found', 0, 'tweets')\n",
      "no tweets found\n",
      "('found', 0, 'tweets')\n",
      "no tweets found\n",
      "('found', 0, 'tweets')\n",
      "no tweets found\n",
      "Name: Yellowstone\n",
      "          Name  LikesCount  RetweetCount\n",
      "0  Yellowstone     1164398        790777\n"
     ]
    }
   ],
   "source": [
    "likes_lst=[]\n",
    "retweets_lst=[]\n",
    "venue_name_lst=[]\n",
    "\n",
    "def main():\n",
    "    ''' This is a script that continuously searches for tweets\n",
    "        that were created over a given number of days. The search\n",
    "        dates and search phrase can be changed below. '''\n",
    "\n",
    "    ''' search variables: '''\n",
    "    search_phrases = ['Isle Royale National Park', 'Joshua Tree National Park', 'Katmai National Park and Preserve', 'Kenai Fjords National Park', 'Kings Canyon National Park', 'Kobuk Valley National Park', 'Lake Clark National Park', 'Lassen Volcanic National Park', 'Mammoth Cave National Park', 'Mesa Verde National Park', 'Mount Rainier National Park', 'National Park of American Samoa', 'North Cascades National Park', 'Olympic National Park', 'Petrified Forest National Park', 'Pinnacles National Park', 'Redwood National Park', 'Rocky Mountain National Park', 'Saguar National Park', 'Sequoia National Park', 'Shenandoah National Park', 'Theodore Roosevelt National Park', 'Virgin Islands National Park', 'Voyageurs National Park', 'Wind Cave National Park', 'Wrangell-St. Elias National Park ad Preserve', 'Yellowstone National Park', 'Yosemite National Park', 'Zion National Park']\n",
    "    #search_phrases = ['Yellowstone'];\n",
    "    time_limit = 1.5                           # runtime limit in hours\n",
    "    max_tweets = 200                           # number of tweets per search (will be\n",
    "                                               # iterated over) - maximum is 100\n",
    "    #min_days_old, max_days_old = 6, 7          # search limits e.g., from 7 to 8\n",
    "    min_days_old, max_days_old = 1,8\n",
    "                                               # gives current weekday from last week,\n",
    "                                               # min_days_old=0 will search from right now\n",
    "    USA = '39.8,-95.583068847656,2500km'       # this geocode includes nearly all American\n",
    "                                               # states (and a large portion of Canada)\n",
    "    \n",
    "\n",
    "    # loop over search items,\n",
    "    # creating a new file for each\n",
    "    for search_phrase in search_phrases:\n",
    "\n",
    "        print \"Search phrase =\",search_phrase\n",
    "\n",
    "        ''' other variables '''\n",
    "        #name = search_phrase.split()[0]\n",
    "        name = search_phrase\n",
    "        json_file_root = name + '/' + name\n",
    "        \n",
    "        if (os.path.exists(name) == False):\n",
    "            os.makedirs(os.path.dirname(json_file_root), 0755)\n",
    "        read_IDs = False\n",
    "        \n",
    "        # open a file in which to store the tweets\n",
    "        if max_days_old - min_days_old == 1:\n",
    "            d = dt.datetime.now() - dt.timedelta(days=min_days_old)\n",
    "            day = '{0}-{1:0>2}-{2:0>2}'.format(d.year, d.month, d.day)\n",
    "        else:\n",
    "            d1 = dt.datetime.now() - dt.timedelta(days=max_days_old-1)\n",
    "            d2 = dt.datetime.now() - dt.timedelta(days=min_days_old)\n",
    "            day = '{0}-{1:0>2}-{2:0>2}_to_{3}-{4:0>2}-{5:0>2}'.format(\n",
    "                  d1.year, d1.month, d1.day, d2.year, d2.month, d2.day)\n",
    "        json_file = json_file_root + '_' + day + '.json'\n",
    "        if os.path.isfile(json_file):\n",
    "            #print('Appending tweets to file named: ',json_file)\n",
    "            read_IDs = True\n",
    "        \n",
    "        # authorize and load the twitter API\n",
    "        api = load_api()\n",
    "        \n",
    "        # set the 'starting point' ID for tweet collection\n",
    "        if read_IDs:\n",
    "            # open the json file and get the latest tweet ID\n",
    "            with open(json_file, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                max_id = json.loads(lines[-1])['id']\n",
    "                #print('Searching from the bottom ID in file')\n",
    "        else:\n",
    "            # get the ID of a tweet that is min_days_old\n",
    "            if min_days_old == 0:\n",
    "                max_id = -1\n",
    "            else:\n",
    "                max_id = get_tweet_id(api, days_ago=(min_days_old-1))\n",
    "        # set the smallest ID to search for\n",
    "        since_id = get_tweet_id(api, days_ago=(max_days_old-1))\n",
    "        #print('max id (starting point) =', max_id)\n",
    "        #print('since id (ending point) =', since_id)\n",
    "        \n",
    "        ''' tweet gathering loop  '''\n",
    "        start = dt.datetime.now()\n",
    "        end = start + dt.timedelta(hours=time_limit)\n",
    "        count, exitcount = 0, 0\n",
    "        #like_count = 0\n",
    "        while dt.datetime.now() < end:\n",
    "            count += 1\n",
    "            #print('count =',count)\n",
    "            # collect tweets and update max_id\n",
    "            tweets, max_id = tweet_search(api, search_phrase, max_tweets,\n",
    "                                          max_id=max_id, since_id=since_id,\n",
    "                                          geocode=USA)\n",
    "\n",
    "            \n",
    "            # write tweets to file in JSON format\n",
    "            if tweets:\n",
    "                write_tweets(tweets, json_file)\n",
    "                exitcount = 0\n",
    "            else:\n",
    "                exitcount += 1\n",
    "                if exitcount == 3:\n",
    "                    if search_phrase == search_phrases[-1]:\n",
    "                        break\n",
    "                    else:\n",
    "                        #print('Maximum number of empty tweet strings reached - breaking')\n",
    "                        break\n",
    "        \n",
    "        file = json_file\n",
    "        tweets = []\n",
    "        if os.path.isfile(json_file):\n",
    "            with open(file, 'r') as f:\n",
    "                    for line in f.readlines():\n",
    "                        tweets.append(json.loads(line))\n",
    "\n",
    "            tweetsDf  = populate_tweet_df(tweets)\n",
    "            aggregate(tweetsDf,search_phrase)\n",
    "        \n",
    "#dataFrame.to_csv(\"twitter.csv\", sep='\\t', encoding='utf-8')\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    \n",
    "venue_df['Name']=venue_name_lst\n",
    "venue_df['LikesCount']=likes_lst\n",
    "venue_df['RetweetCount']=retweets_lst\n",
    "\n",
    "print venue_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "venue_df.to_csv(\"twitter.csv\", sep= '\\t' , encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweetsDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-cbe406a45373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mprint\u001b[0m \u001b[0mtweetsDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tweetsDf' is not defined"
     ]
    }
   ],
   "source": [
    "print tweetsDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists\n"
     ]
    }
   ],
   "source": [
    "name = 'Yellowstone'\n",
    "    \n",
    "if (os.path.exists(name) == False):\n",
    "    print \"Does not exist\"\n",
    "else:\n",
    "    print \"Exists\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
